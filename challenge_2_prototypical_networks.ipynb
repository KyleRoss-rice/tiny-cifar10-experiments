{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prototypical_networks_challenge_two",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyleRoss-rice/tiny-cifar10-experiments/blob/main/challenge_2_prototypical_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i07JHz1wtRq",
        "outputId": "f47bb5d4-7630-4bec-ccc1-f96a129ae6e6"
      },
      "source": [
        "!pip install learn2learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: learn2learn in /usr/local/lib/python3.7/dist-packages (0.1.5)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from learn2learn) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from learn2learn) (0.9.1+cu101)\n",
            "Requirement already satisfied: gsutil in /usr/local/lib/python3.7/dist-packages (from learn2learn) (4.61)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from learn2learn) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from learn2learn) (1.1.5)\n",
            "Requirement already satisfied: gym>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from learn2learn) (0.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from learn2learn) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from learn2learn) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->learn2learn) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->learn2learn) (7.1.2)\n",
            "Requirement already satisfied: argcomplete>=1.9.4 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (1.12.3)\n",
            "Requirement already satisfied: retry-decorator>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (1.1.1)\n",
            "Requirement already satisfied: google-apitools>=0.5.30 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (0.5.31)\n",
            "Requirement already satisfied: httplib2>=0.18 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (0.19.1)\n",
            "Requirement already satisfied: pyOpenSSL>=0.13 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (20.0.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (1.15.0)\n",
            "Requirement already satisfied: gcs-oauth2-boto-plugin>=2.7 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (2.7)\n",
            "Requirement already satisfied: google-reauth>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (0.1.1)\n",
            "Requirement already satisfied: mock==2.0.0 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (2.0.0)\n",
            "Requirement already satisfied: monotonic>=1.4 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (1.6)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (1.7)\n",
            "Requirement already satisfied: fasteners>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from gsutil->learn2learn) (0.16)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->learn2learn) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->learn2learn) (2018.9)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.14.0->learn2learn) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.14.0->learn2learn) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.14.0->learn2learn) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->learn2learn) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->learn2learn) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->learn2learn) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->learn2learn) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata<5,>=0.23; python_version == \"3.7\" in /usr/local/lib/python3.7/dist-packages (from argcomplete>=1.9.4->gsutil->learn2learn) (3.10.1)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from google-apitools>=0.5.30->gsutil->learn2learn) (4.1.3)\n",
            "Requirement already satisfied: pyparsing<3,>=2.4.2 in /usr/local/lib/python3.7/dist-packages (from httplib2>=0.18->gsutil->learn2learn) (2.4.7)\n",
            "Requirement already satisfied: cryptography>=3.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.13->gsutil->learn2learn) (3.4.7)\n",
            "Requirement already satisfied: boto>=2.29.1 in /usr/local/lib/python3.7/dist-packages (from gcs-oauth2-boto-plugin>=2.7->gsutil->learn2learn) (2.49.0)\n",
            "Requirement already satisfied: pyu2f in /usr/local/lib/python3.7/dist-packages (from google-reauth>=0.1.0->gsutil->learn2learn) (0.1.5)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.7/dist-packages (from mock==2.0.0->gsutil->learn2learn) (5.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.14.0->learn2learn) (0.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5,>=0.23; python_version == \"3.7\"->argcomplete>=1.9.4->gsutil->learn2learn) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=1.4.12->google-apitools>=0.5.30->gsutil->learn2learn) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=1.4.12->google-apitools>=0.5.30->gsutil->learn2learn) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=1.4.12->google-apitools>=0.5.30->gsutil->learn2learn) (4.7.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.2->pyOpenSSL>=0.13->gsutil->learn2learn) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.2->pyOpenSSL>=0.13->gsutil->learn2learn) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiRWamQAzPzO"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torchvision\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import RandomState\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "import learn2learn as l2l\n",
        "from learn2learn.data.transforms import NWays, KShots, LoadData, RemapLabels\n",
        "from learn2learn.vision.datasets import MiniImagenet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB3gzBAj_xJp"
      },
      "source": [
        "#Useful Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX_MoNP6_klv"
      },
      "source": [
        "def pairwise_distances_logits(a, b):                                            #  L2 norm of each query point and the prototype of each class \n",
        "    n = a.shape[0]\n",
        "    \n",
        "    m = b.shape[0]\n",
        "   \n",
        "    logits = -((a.unsqueeze(1).expand(n, m, -1) -b.unsqueeze(0).expand(n, m, -1))**2).sum(dim=2) # (query points x way (class) x embed_dim ) summed over embed_dim\n",
        "  \n",
        "    return logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAx2xjDJ_mtI"
      },
      "source": [
        "def accuracy(predictions, targets):\n",
        "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
        "    return (predictions == targets).sum().float() / targets.size(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXorBuTe_jBh"
      },
      "source": [
        "def fast_adapt(model, batch, ways, shot, query_num, metric=None, device=None):\n",
        "    if metric is None:\n",
        "        metric = pairwise_distances_logits\n",
        "    if device is None:\n",
        "        device = model.device()\n",
        "    data, labels = batch\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    n_items = shot * ways                                                       # way== class and shot== number of samples per class, total samples would be class x # of samples/class\n",
        "\n",
        "    # Sort data samples by labels\n",
        "    sort = torch.sort(labels)                                                   # so that images from the same class are near each other \n",
        "    data = data.squeeze(0)[sort.indices].squeeze(0)\n",
        "    labels = labels.squeeze(0)[sort.indices].squeeze(0)\n",
        "   \n",
        "    # Compute support and query embeddings\n",
        "    embeddings = model(data)                                                    # The model learns an embedding space each image is mapped to a 1x256 tensor \n",
        "    \n",
        "    support_indices = np.zeros(data.size(0), dtype=bool)                        # initialize all False \n",
        "   \n",
        "    selection = np.arange(ways) * (shot + query_num)                            # select some of the data to be part of the support set \n",
        "    \n",
        "    for offset in range(shot):                                                  # Shots from each class \n",
        "        support_indices[selection + offset] = True                              # Set only the indices of the selected data (support) to be True\n",
        "        \n",
        "    query_indices = torch.from_numpy(~support_indices)                          \n",
        "    support_indices = torch.from_numpy(support_indices)\n",
        "    \n",
        "    support = embeddings[support_indices]                                       # The support set\n",
        "    \n",
        "    support = support.reshape(ways, shot, -1).mean(dim=1)                       # A class' prototype representation is the mean vector of the support set in the learned embedding space\n",
        "                                                                                # Each way (class) will have a prototype, hence the shape here would be way x embed_dim (256)\n",
        "\n",
        "    \n",
        "    query = embeddings[query_indices]                                           # The query set \n",
        "    \n",
        "    labels = labels[query_indices].long()\n",
        "\n",
        "    logits = pairwise_distances_logits(query, support)                          # Calculating the distance between prototype representation (obtained by the model) and query point \n",
        "                                                                                # The distances will be of shape query points x ways \n",
        "    loss = F.cross_entropy(logits, labels)                                      \n",
        "    acc = accuracy(logits, labels)\n",
        "    return loss, acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQnZLuF7_owJ"
      },
      "source": [
        "# The model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TebnENk_oKH"
      },
      "source": [
        "class Convnet(nn.Module):\n",
        "\n",
        "    def __init__(self, x_dim=3, hid_dim=64, z_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = l2l.vision.models.ConvBase(output_size=z_dim,\n",
        "                                                  hidden=hid_dim,\n",
        "                                                  channels=x_dim,\n",
        "                                                  max_pool=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x.view(x.size(0), -1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CFZQWtM_vpt"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEacUmuGwixU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5068296f-2676-4be4-c23d-2ee64a4cba9b"
      },
      "source": [
        "max_epoch =250\n",
        "\n",
        "train_shot = 1   # Train shots (number of samples that will be used as support -- used to calculate the prototypes)\n",
        "valid_shot = 1   # In our experiments we chose to keep the number of shots the same for all (train, validation, and testing)\n",
        "test_shot = 1\n",
        "\n",
        "train_way = 10   # 10-way since the CIFAR-FS has 100 classes  ( # of prototypes == # of ways ), higher ways in training increases the accuracy and helps the network find better embeddings -- network generalizes better (reference prototypical networks)\n",
        "valid_way = 5    #  5-way on CIFAR-10\n",
        "test_way = 5     # (to be decided on)\n",
        "\n",
        "train_query = 15 # 15 query points per episode (the more query points we have the more training we are doing) -- similar to what they did in the prototypical networks with mini-imagenet\n",
        "valid_query = 5  # Since each way/class has 10 images in our subset of 100 samples from CIFAR-10\n",
        "test_query = 5   # (to decided on)\n",
        "\n",
        "gpu=True\n",
        "device = torch.device('cpu')\n",
        "if gpu and torch.cuda.device_count():\n",
        "    print(\"Using gpu\")\n",
        "    torch.cuda.manual_seed(43)\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using gpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8hi3jBnmuoB",
        "outputId": "7ae9e5c6-0b4e-40d5-80bd-6995b1837838"
      },
      "source": [
        "## Test set I : 2000 samples from CIFAR-10 training data\n",
        "## Test set II: 2000 samples from CIFAR-10 testing data\n",
        "n_subsets=3                                                                     # 3 different subsets will be used to calculate the mean and std for the testing accuracy\n",
        "accs =[]\n",
        "final_accs=[]\n",
        "torch.cuda.manual_seed(43)\n",
        "for i in range(n_subsets):\n",
        "  # normalize the data\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "  transf = transforms.Compose([transforms.ToTensor(), normalize]) \n",
        "  \n",
        "\n",
        "  # download datasets\n",
        "  cifar_data_val = datasets.CIFAR10(root='datasets', train=True, transform=transf, download=True)\n",
        "  cifar_data_test = datasets.CIFAR10(root='datasets', train=True, transform=transf, download=True)  # Used to construct test set I\n",
        "  final_test_data = datasets.CIFAR10(root='datasets', train=False, transform=transf, download=True) # Used to construct test set II\n",
        "  prng = RandomState(i)\n",
        "  random_permute = prng.permutation(np.arange(0, 5000))\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data_val.targets) == classe)[0][random_permute[0:10]] for classe in range(0, 10)])\n",
        "  indx_tst = np.concatenate([np.where(np.array(cifar_data_test.targets) == classe)[0][random_permute[10:210]] for classe in range(0, 10)])\n",
        "  random_permute = prng.permutation(np.arange(0, 900))\n",
        "  tst = np.concatenate([np.where(np.array(final_test_data.targets) == classe)[0][random_permute[0:200]] for classe in range(0, 10)])\n",
        "  \n",
        "  train_dataset = l2l.vision.datasets.CIFARFS(root='./data', mode='train',download=True,transform=transforms.Compose([transforms.ToTensor()])) \n",
        "  valid_dataset= Subset(cifar_data_val, indx_val)   # Validation data (100 samples from CIFAR-10 training data)\n",
        "  test_dataset=  Subset(cifar_data_test, indx_tst)  # Test set I (2000 samples from CIFAR-10 training data)\n",
        "  final_test_dataset = Subset(final_test_data,tst)  # Test set II (2000 samples from CIFAR-10 testing data)\n",
        "  \n",
        "  # Setting up the meta-training datasets\n",
        "  train_dataset = l2l.data.MetaDataset(train_dataset)\n",
        "  train_transforms = [\n",
        "      NWays(train_dataset, train_way),    # N-way = number of classes\n",
        "      KShots(train_dataset, train_query + train_shot), # k-shots = number of samples per class, the shot is used in the support set and the query is for validation\n",
        "      LoadData(train_dataset),\n",
        "      RemapLabels(train_dataset)\n",
        "      \n",
        "  ]\n",
        "  train_tasks = l2l.data.TaskDataset(train_dataset, task_transforms=train_transforms)\n",
        "  train_loader = DataLoader(train_tasks, pin_memory=True, shuffle=True)\n",
        "\n",
        "  valid_dataset = l2l.data.MetaDataset(valid_dataset)\n",
        "  valid_transforms = [\n",
        "      NWays(valid_dataset, valid_way),\n",
        "      KShots(valid_dataset, valid_query + valid_shot),\n",
        "      LoadData(valid_dataset),\n",
        "      RemapLabels(valid_dataset)\n",
        "  ]\n",
        "  valid_tasks = l2l.data.TaskDataset(valid_dataset,\n",
        "                                      task_transforms=valid_transforms\n",
        "                                      ,num_tasks=10)                            \n",
        "  valid_loader = DataLoader(valid_tasks, pin_memory=True, shuffle=True)\n",
        "\n",
        "  test_dataset = l2l.data.MetaDataset(test_dataset)\n",
        "  test_transforms = [\n",
        "      NWays(test_dataset, test_way),\n",
        "      KShots(test_dataset,test_query + test_shot),\n",
        "      LoadData(test_dataset),\n",
        "      RemapLabels(test_dataset),\n",
        "  ]\n",
        "  test_tasks = l2l.data.TaskDataset(test_dataset,\n",
        "                                    task_transforms=test_transforms,num_tasks=10)\n",
        "  test_loader = DataLoader(test_tasks, pin_memory=True, shuffle=True)\n",
        "  \n",
        "  final_test_dataset = l2l.data.MetaDataset(final_test_dataset)\n",
        "  final_test_transforms = [\n",
        "      NWays(final_test_dataset, test_way),\n",
        "      KShots(final_test_dataset,test_query + test_shot),\n",
        "      LoadData(final_test_dataset),\n",
        "      RemapLabels(final_test_dataset),\n",
        "  ]\n",
        "  final_test_tasks = l2l.data.TaskDataset(final_test_dataset,\n",
        "                                    task_transforms=final_test_transforms,num_tasks=10)\n",
        "  final_test_loader = DataLoader(final_test_tasks, pin_memory=True, shuffle=True)\n",
        "  torch.cuda.manual_seed(43)\n",
        "  \n",
        "  ################################# Training ###################################\n",
        "  \n",
        "  model =  Convnet()\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "      optimizer, step_size=20, gamma=0.5)\n",
        "  train_losses =[]\n",
        "  train_acc =[]\n",
        "  valid_losses=[]\n",
        "  valid_acc=[]\n",
        "\n",
        "  for epoch in range(1, 21):\n",
        "    model.train()\n",
        "\n",
        "    loss_ctr = 0\n",
        "    n_loss = 0\n",
        "    n_acc = 0\n",
        "\n",
        "    for i in range(100):                                                          # 100 tasks -- the tasks are created on the fly\n",
        "        batch = next(iter(train_loader))\n",
        "\n",
        "        loss, acc = fast_adapt(model,\n",
        "                                batch,\n",
        "                                train_way,\n",
        "                                train_shot,\n",
        "                                train_query,\n",
        "                                metric=pairwise_distances_logits,\n",
        "                                device=device)\n",
        "        \n",
        "        loss_ctr += 1\n",
        "        n_loss += loss.item()\n",
        "        n_acc += acc\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_losses.append(loss)\n",
        "    train_acc.append(acc)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    print('epoch {}, train, loss={:.4f} acc={:.4f}'.format(\n",
        "        epoch, n_loss/loss_ctr, n_acc/loss_ctr))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    loss_ctr = 0\n",
        "    n_loss = 0\n",
        "    n_acc = 0\n",
        "    for i, batch in enumerate(valid_loader):\n",
        "        loss, acc = fast_adapt(model,\n",
        "                                batch,\n",
        "                                valid_way,\n",
        "                                valid_shot,\n",
        "                                valid_query,\n",
        "                                metric=pairwise_distances_logits,\n",
        "                                device=device)\n",
        "        \n",
        "        loss_ctr += 1\n",
        "        n_loss += loss.item()\n",
        "        n_acc += acc\n",
        "    valid_losses.append(loss)\n",
        "    valid_acc.append(acc)\n",
        "    print('epoch {}, val, loss={:.4f} acc={:.4f}'.format(\n",
        "        epoch, n_loss/loss_ctr, n_acc/loss_ctr))\n",
        "  model.eval()\n",
        "  loss_ctr = 0\n",
        "  n_acc = 0\n",
        "\n",
        "  for i, batch in enumerate(test_loader, 1):\n",
        "    loss, acc = fast_adapt(model,\n",
        "                            batch,\n",
        "                            test_way,\n",
        "                            test_shot,\n",
        "                            test_query,\n",
        "                            metric=pairwise_distances_logits,\n",
        "                            device=device)\n",
        "    loss_ctr += 1\n",
        "    n_acc += acc\n",
        "  print(f\"The testing accuracy using test set I (2000 CIFAR-10 training samples) is: {np.round(int((n_acc*2000/loss_ctr).cpu().detach().numpy()))}/2000 {n_acc*100/loss_ctr:.2f}% \")\n",
        "  accs.append((n_acc*100/loss_ctr).item())\n",
        "\n",
        "  model.eval()\n",
        "  loss_ctr = 0\n",
        "  n_acc = 0\n",
        "\n",
        "  for i, batch in enumerate(final_test_loader, 1):\n",
        "    loss, acc = fast_adapt(model,\n",
        "                            batch,\n",
        "                            test_way,\n",
        "                            test_shot,\n",
        "                            test_query,\n",
        "                            metric=pairwise_distances_logits,\n",
        "                            device=device)\n",
        "    loss_ctr += 1\n",
        "    n_acc += acc\n",
        "  print(f\"The testing accuracy using test set II (2000 CIFAR-10 testing samples) is: {np.round(int((n_acc*2000/loss_ctr).cpu().detach().numpy()))}/2000 {n_acc*100/loss_ctr:.2f}% \")\n",
        "  final_accs.append((n_acc*100/loss_ctr).item())\n",
        "accs= np.array(accs)\n",
        "print(f\"Acc over {n_subsets} instances of test set I is: {accs.mean():.3f} +- {accs.std():.3f}\")\n",
        "final_accs= np.array(final_accs)\n",
        "print(f\"Acc over {n_subsets} instances of test set II is: {final_accs.mean():.3f} +- {final_accs.std():.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "epoch 1, train, loss=2.6878 acc=0.2467\n",
            "epoch 1, val, loss=4.0647 acc=0.2920\n",
            "epoch 2, train, loss=2.1222 acc=0.2432\n",
            "epoch 2, val, loss=3.2027 acc=0.2960\n",
            "epoch 3, train, loss=2.0954 acc=0.2477\n",
            "epoch 3, val, loss=3.2423 acc=0.3160\n",
            "epoch 4, train, loss=2.0465 acc=0.2785\n",
            "epoch 4, val, loss=2.7669 acc=0.2920\n",
            "epoch 5, train, loss=2.0617 acc=0.2676\n",
            "epoch 5, val, loss=2.6553 acc=0.3120\n",
            "epoch 6, train, loss=2.0538 acc=0.2600\n",
            "epoch 6, val, loss=2.6660 acc=0.3120\n",
            "epoch 7, train, loss=2.0314 acc=0.2777\n",
            "epoch 7, val, loss=2.0929 acc=0.3360\n",
            "epoch 8, train, loss=1.9750 acc=0.3018\n",
            "epoch 8, val, loss=2.3334 acc=0.3480\n",
            "epoch 9, train, loss=1.9762 acc=0.2992\n",
            "epoch 9, val, loss=2.3298 acc=0.3440\n",
            "epoch 10, train, loss=1.9597 acc=0.3032\n",
            "epoch 10, val, loss=2.1469 acc=0.3720\n",
            "epoch 11, train, loss=1.9751 acc=0.3057\n",
            "epoch 11, val, loss=2.1219 acc=0.3360\n",
            "epoch 12, train, loss=1.9436 acc=0.3097\n",
            "epoch 12, val, loss=2.0999 acc=0.3480\n",
            "epoch 13, train, loss=1.9320 acc=0.3239\n",
            "epoch 13, val, loss=2.4148 acc=0.3240\n",
            "epoch 14, train, loss=1.9463 acc=0.3082\n",
            "epoch 14, val, loss=2.1647 acc=0.3320\n",
            "epoch 15, train, loss=1.9001 acc=0.3268\n",
            "epoch 15, val, loss=1.8746 acc=0.3360\n",
            "epoch 16, train, loss=1.9087 acc=0.3262\n",
            "epoch 16, val, loss=2.2447 acc=0.3400\n",
            "epoch 17, train, loss=1.9058 acc=0.3217\n",
            "epoch 17, val, loss=2.5072 acc=0.3040\n",
            "epoch 18, train, loss=1.8693 acc=0.3381\n",
            "epoch 18, val, loss=1.8226 acc=0.3640\n",
            "epoch 19, train, loss=1.8655 acc=0.3494\n",
            "epoch 19, val, loss=2.1856 acc=0.3520\n",
            "epoch 20, train, loss=1.8931 acc=0.3328\n",
            "epoch 20, val, loss=2.1214 acc=0.3800\n",
            "The testing accuracy using test set I (2000 CIFAR-10 training samples) is: 672/2000 33.60% \n",
            "The testing accuracy using test set II (2000 CIFAR-10 testing samples) is: 608/2000 30.40% \n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "epoch 1, train, loss=2.7829 acc=0.2235\n",
            "epoch 1, val, loss=9.8050 acc=0.2600\n",
            "epoch 2, train, loss=2.1155 acc=0.2407\n",
            "epoch 2, val, loss=4.4709 acc=0.2560\n",
            "epoch 3, train, loss=2.1158 acc=0.2367\n",
            "epoch 3, val, loss=3.6997 acc=0.2800\n",
            "epoch 4, train, loss=2.0570 acc=0.2656\n",
            "epoch 4, val, loss=3.0129 acc=0.2800\n",
            "epoch 5, train, loss=2.0267 acc=0.2814\n",
            "epoch 5, val, loss=2.9572 acc=0.3000\n",
            "epoch 6, train, loss=2.0218 acc=0.2881\n",
            "epoch 6, val, loss=2.6080 acc=0.3080\n",
            "epoch 7, train, loss=2.0221 acc=0.2799\n",
            "epoch 7, val, loss=2.5516 acc=0.3160\n",
            "epoch 8, train, loss=2.0100 acc=0.2891\n",
            "epoch 8, val, loss=1.9442 acc=0.3360\n",
            "epoch 9, train, loss=2.0040 acc=0.2879\n",
            "epoch 9, val, loss=2.1436 acc=0.2640\n",
            "epoch 10, train, loss=1.9787 acc=0.3010\n",
            "epoch 10, val, loss=2.1998 acc=0.2760\n",
            "epoch 11, train, loss=1.9599 acc=0.2966\n",
            "epoch 11, val, loss=2.1740 acc=0.2840\n",
            "epoch 12, train, loss=1.9580 acc=0.3083\n",
            "epoch 12, val, loss=2.1837 acc=0.2680\n",
            "epoch 13, train, loss=1.9393 acc=0.3206\n",
            "epoch 13, val, loss=1.7300 acc=0.3040\n",
            "epoch 14, train, loss=1.9159 acc=0.3188\n",
            "epoch 14, val, loss=1.9534 acc=0.2960\n",
            "epoch 15, train, loss=1.9310 acc=0.3129\n",
            "epoch 15, val, loss=2.1814 acc=0.3240\n",
            "epoch 16, train, loss=1.8921 acc=0.3306\n",
            "epoch 16, val, loss=2.2397 acc=0.3280\n",
            "epoch 17, train, loss=1.9069 acc=0.3228\n",
            "epoch 17, val, loss=1.7962 acc=0.3520\n",
            "epoch 18, train, loss=1.9001 acc=0.3258\n",
            "epoch 18, val, loss=1.8145 acc=0.3160\n",
            "epoch 19, train, loss=1.8870 acc=0.3286\n",
            "epoch 19, val, loss=1.8075 acc=0.3960\n",
            "epoch 20, train, loss=1.8887 acc=0.3354\n",
            "epoch 20, val, loss=1.8987 acc=0.3200\n",
            "The testing accuracy using test set I (2000 CIFAR-10 training samples) is: 663/2000 33.20% \n",
            "The testing accuracy using test set II (2000 CIFAR-10 testing samples) is: 536/2000 26.80% \n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "epoch 1, train, loss=2.5324 acc=0.2300\n",
            "epoch 1, val, loss=2.4444 acc=0.2960\n",
            "epoch 2, train, loss=2.1067 acc=0.2503\n",
            "epoch 2, val, loss=2.1442 acc=0.2920\n",
            "epoch 3, train, loss=2.0669 acc=0.2632\n",
            "epoch 3, val, loss=2.4349 acc=0.2880\n",
            "epoch 4, train, loss=2.0552 acc=0.2703\n",
            "epoch 4, val, loss=2.2420 acc=0.2840\n",
            "epoch 5, train, loss=2.0006 acc=0.2843\n",
            "epoch 5, val, loss=2.4969 acc=0.3080\n",
            "epoch 6, train, loss=2.0192 acc=0.2865\n",
            "epoch 6, val, loss=2.2345 acc=0.2680\n",
            "epoch 7, train, loss=2.0330 acc=0.2794\n",
            "epoch 7, val, loss=1.9293 acc=0.3400\n",
            "epoch 8, train, loss=1.9502 acc=0.3077\n",
            "epoch 8, val, loss=2.3817 acc=0.2880\n",
            "epoch 9, train, loss=1.9521 acc=0.3135\n",
            "epoch 9, val, loss=2.9981 acc=0.2720\n",
            "epoch 10, train, loss=1.9609 acc=0.3092\n",
            "epoch 10, val, loss=2.0589 acc=0.2920\n",
            "epoch 11, train, loss=1.9545 acc=0.3100\n",
            "epoch 11, val, loss=2.0685 acc=0.2800\n",
            "epoch 12, train, loss=1.9408 acc=0.3144\n",
            "epoch 12, val, loss=1.8805 acc=0.3200\n",
            "epoch 13, train, loss=1.9166 acc=0.3212\n",
            "epoch 13, val, loss=1.9856 acc=0.3000\n",
            "epoch 14, train, loss=1.9235 acc=0.3281\n",
            "epoch 14, val, loss=2.1159 acc=0.3080\n",
            "epoch 15, train, loss=1.9203 acc=0.3180\n",
            "epoch 15, val, loss=1.9484 acc=0.3040\n",
            "epoch 16, train, loss=1.9172 acc=0.3205\n",
            "epoch 16, val, loss=2.1786 acc=0.3080\n",
            "epoch 17, train, loss=1.8972 acc=0.3290\n",
            "epoch 17, val, loss=1.9356 acc=0.3080\n",
            "epoch 18, train, loss=1.8734 acc=0.3333\n",
            "epoch 18, val, loss=2.0330 acc=0.2960\n",
            "epoch 19, train, loss=1.8977 acc=0.3287\n",
            "epoch 19, val, loss=1.9783 acc=0.3280\n",
            "epoch 20, train, loss=1.8742 acc=0.3399\n",
            "epoch 20, val, loss=1.9418 acc=0.3000\n",
            "The testing accuracy using test set I (2000 CIFAR-10 training samples) is: 592/2000 29.60% \n",
            "The testing accuracy using test set II (2000 CIFAR-10 testing samples) is: 647/2000 32.40% \n",
            "Acc over 3 instances of test set I is: 32.133 +- 1.799\n",
            "Acc over 3 instances of test set II is: 29.867 +- 2.317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8smEHUYFGkeE"
      },
      "source": [
        "# Calculating testing accuracy using a similar approach to the \"fast_adapt\" function (Sanity check suggested by Dr. Eugene)\n",
        "\n",
        "In this section we try to calculate the testing accuracy using the following approach.\n",
        "  \n",
        "  1- First embed the 100 samples from the CIFAR-10 training data using the trained embedding. \n",
        " \n",
        "  2- Then we calculate a class' prototypes by averaging the image embedddings\n",
        " \n",
        "  3- The distance between an embedded test image and the 10 prototypes is measured, and the label of closest prototype is assigned to the test image.\n",
        "\n",
        "\n",
        "  Note: This section is only for sanity checking and was only done on one experiment (10-way 1-shot) for only one subset of test set I"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Beh8bp33Ng8w"
      },
      "source": [
        "dataloaders = (torch.utils.data.DataLoader(valid_dataset, batch_size=100, shuffle=True),\n",
        "                    torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYsidoYOGnqj"
      },
      "source": [
        "model.eval()\n",
        "data_iter = iter(dataloaders[0])\n",
        "data, target = next(data_iter)\n",
        "data, target = data.to(device), target.to(device)\n",
        "with torch.no_grad():\n",
        "  embeddings = model(data)                      # Embedding the 100 samples from CIFAR-10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqzbJM5BIOD2"
      },
      "source": [
        "prototypes = torch.empty((10,256),dtype=torch.float32)\n",
        "for i in range(10):\n",
        "  prototypes[i,:] = embeddings[np.where(target.cpu()==i)].mean(axis=0)          # A class' prototype is the average of the embeddings of the images in the class\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5qwzJ7gJGH6",
        "outputId": "6102a684-d765-45fb-e20c-405b567ab786"
      },
      "source": [
        "model.to(\"cpu\")\n",
        "device = \"cpu\"\n",
        "correct = 0\n",
        "for id, (x,y) in enumerate(dataloaders[1]):\n",
        "  x,y = x.to(device), y.to(device)\n",
        "  prototypes.to(device)\n",
        "  with torch.no_grad():\n",
        "    dist = torch.norm(prototypes - model(x), dim=1, p=None)                     # Calculating the distance between the embedded test image and 10 prototypes (10 classes)\n",
        "    knn = dist.topk(5, largest=False)                                           # Getting the closest 5 prototypes of the test image\n",
        "    prediction = knn.indices[knn.values.argmin()]                               # The assigned label is for the closest prototype to the test image\n",
        "    correct += prediction.eq(y.view_as(prediction)).sum().item()                # determining the number of correctly classified test images\n",
        "print(f\"The accuracy is {correct*100/len(dataloaders[1]):.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 28.30%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}